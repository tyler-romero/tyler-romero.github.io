<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Tyler’s Technical Blog</title>
<meta name="author" content="Tyler Romero">
<meta name="description" content="Notes on Machine Learning and related topics.">
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/assets/img/favicon.ico" type="image/ico">
<link rel="canonical" href="https://www.tylerromero.com/">
<link type="application/atom+xml" rel="alternate" href="https://www.tylerromero.com/feed.xml" title="Tyler&#39;s Technical Blog">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,300..700;1,300..700&display=swap">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff">
<!-- Open Graph -->
<meta property="og:title" content="Tyler’s Technical Blog">
<meta property="og:description" content="Notes on Machine Learning and related topics.">
<meta property="og:url" content="https://www.tylerromero.com/">
<meta property="og:type" content="website">
<meta property="og:site_name" content="Tyler&#39;s Technical Blog">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tyler’s Technical Blog">
<meta name="twitter:description" content="Notes on Machine Learning and related topics.">



<!-- KaTeX Support -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<!-- BibTeX Support -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js"></script>
<!-- Stylesheets -->
<link rel="stylesheet" href="/assets/tufte.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
<!-- Goat Counter for basic view counting w/o cookies -->
<script data-goatcounter="https://tylerromero.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

        
        <script type="application/ld+json">
            {
            "@context": "https://schema.org",
            "@type": "WebSite",
            "name": "Tyler&#39;s Technical Blog",
            "headline": "",
            "description": "Notes on Machine Learning and related topics.",
            "author": {
                "@type": "Person",
                "name": "Tyler Romero",
                "url": "https://www.tylerromero.com"
            },
            "dateModified": "2026-02-18T01:42:33.786Z",
            "url": "https://www.tylerromero.com/",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.tylerromero.com/"
            },
            "image": "https://www.tylerromero.com",
            "keywords": "ML, Research, Blog, LLMs"
            }
        </script>

    </head>
    <body>
        <header>
            <nav class="topbar">
                <div class="right-aligned-links">
                    <a href="/" class="no-tufte-underline nav-brand">Tyler Romero</a>
                    
                    <a href="/#posts" class="no-tufte-underline">posts</a>
                    <span class="desktop-only">
                        <a href="/#projects" class="no-tufte-underline">projects</a>
                        <a href="/#reading-list" class="no-tufte-underline">reading list</a>
                    </span>
                    
                </div>
            </nav>
        </header>
        <article>
            
            
<section>
    <h1>Hi, I'm Tyler Romero.</h1>
    <p>
        <span class="marginnote">
            <picture><source type="image/webp" srcset="/assets/img/fYf4w0FgmW-300.webp 300w, /assets/img/fYf4w0FgmW-600.webp 600w, /assets/img/fYf4w0FgmW-702.webp 702w" sizes="(max-width: 900px) 100vw, 900px"><img loading="lazy" decoding="async" class="profile-picture responsive-image" src="/assets/img/fYf4w0FgmW-300.jpeg" alt="me" width="702" height="709" srcset="/assets/img/fYf4w0FgmW-300.jpeg 300w, /assets/img/fYf4w0FgmW-600.jpeg 600w, /assets/img/fYf4w0FgmW-702.jpeg 702w" sizes="(max-width: 900px) 100vw, 900px"></picture>
        </span>I am a Research Engineer at <a href="https://allenai.org/about">the Allen Institute (Ai2)</a>, where I work on <a href="https://allenai.org/language-models">open language modeling</a>.
    </p>
    <p>
        Previously, I was Lead ML Engineer at <a href="https://www.groundlight.ai/">Groundlight</a>, a startup building multimodal question-answering systems. Prior to that, I worked on large-scale recommender systems at Twitter, where I developed the ML and ran the A/B tests for the experimentally-successful yet short-lived <a href="https://techcrunch.com/2021/07/21/twitter-tests-reddit-style-upvote-and-downvote-buttons/">downvote button</a>. I also researched, trained, and shipped model architecture improvements for ranking Twitter's home timeline and conversation reply trees.
        <label for="sd-less-than-a-href" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="sd-less-than-a-href" class="margin-toggle">
        <span class="sidenote"><a href="https://github.com/twitter/the-algorithm-ml/tree/main/projects/home/recap">And some of my work at Twitter is now open-source!</a> Although the git-blame has been sanitized.</span> Earlier in my career, I worked as a Research Scientist at Microsoft, where I built greenfield ML projects.
    </p>
    <p>
        My academic background includes a Master’s in computer science and machine learning from Stanford, and a <a href="https://engineering.tamu.edu/news/2015/10/look-college-honors-outstanding-seniors.html">Bachelor’s in computer engineering</a> from Texas A&#x26;M. As an undergraduate, I performed research on novel implementations of parallel algorithms written in C / <a href="https://cilk.mit.edu/programming/">Cilk</a> and interned as a Software Engineer at Bloomberg and Microsoft.
        <label for="sd-i-made-a-few" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="sd-i-made-a-few" class="margin-toggle">
        <span class="sidenote">I made a few contributions to Bloomberg’s <a href="https://www.bloomberg.com/professional/product/asset-and-investment-manager/">Asset and Investment Management</a> function and wrote Microsoft <a href="https://learn.microsoft.com/en-us/sql/machine-learning/r/ref-r-olapr">a data retrieval package for R</a> that is still supported 8 years later.</span>
    </p>
    <p>
        You can reach me via <a href="mailto:tyler.alexander.romero+site@gmail.com">email</a> or <a href="https://www.linkedin.com/in/tylerromero/">LinkedIn</a> message.
    </p>
</section>
<hr class="section-divider">
<section>
    <h2 id="posts">Posts</h2>
    <ul class="post-list">
        
            <li>
                <span class="post-date">January 10, 2026</span>
                <div class="post-info">
                    <a href="/posts/2026-01-scaling-laws/">Thinking about Scaling Laws</a>
                    <p class="item-description">
                        <small>How can we use scaling laws to train stronger LLMs?</small>
                    </p>
                </div>
            </li>
        
            <li>
                <span class="post-date">March 8, 2025</span>
                <div class="post-info">
                    <a href="/posts/nanogpt-speedrun-worklog/">NanoGPT Speedrun Living Worklog</a>
                    <p class="item-description">
                        <small>How fast can I train GPT-2 on two RTX 4090 GPUs? This is a living worklog of my progress.</small>
                    </p>
                </div>
            </li>
        
            <li>
                <span class="post-date">February 6, 2025</span>
                <div class="post-info">
                    <a href="/posts/2025-02-selective-log-softmax/">Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax</a>
                    <p class="item-description">
                        <small>Reduce VRAM usage by half when computing log probabilities by selectively applying log-softmax to only the necessary tokens. Perfect for many RLHF post-training algorithms (such as PPO and GRPO) where typically only one token&#39;s log probability is needed from the entire vocabulary at each sequence position.</small>
                    </p>
                </div>
            </li>
        
            <li>
                <span class="post-date">January 5, 2025</span>
                <div class="post-info">
                    <a href="/posts/2025-01-badge-extension/">An Extension to BADGE Active Learning for Variable-Sized Batches</a>
                    <p class="item-description">
                        <small>We show how BADGE&#39;s batch selection strategy can be adapted to handle flexible batch sizes without compromising its ability to select diverse, informative samples - enabling more practical active learning workflows.</small>
                    </p>
                </div>
            </li>
        
            <li>
                <span class="post-date">April 13, 2024</span>
                <div class="post-info">
                    <a href="/posts/2024-04-dpo/">Direct Preference Optimization Explained In-depth</a>
                    <p class="item-description">
                        <small>Covering DPO, a recently-proposed alternative to RLHF for preference tuning.</small>
                    </p>
                </div>
            </li>
        
    </ul>
</section>
<hr class="section-divider">
<section>
    <h2 id="publications">Publications & Preprints</h2>
    <ul class="post-list">
        <li>
            <span class="post-date">2025</span>
            <div class="post-info">
                <a href="https://arxiv.org/abs/2512.13961"><strong>Olmo 3</strong></a> <small>— Team Olmo. <em>arXiv preprint arXiv:2512.13961</em>, 2025.</small>
                <p class="item-description">
                    <small>We introduce Olmo 3, a family of state-of-the-art, fully-open language models at the 7B and 32B parameter scales.
                        <label for="sd-olmo3" class="margin-toggle sidenote-number"></label>
                        <input type="checkbox" id="sd-olmo3" class="margin-toggle">
                        <span class="sidenote">Check out <a href="https://huggingface.co/allenai/OLMo-3-32B-Think">Olmo 3 32B Think on HuggingFace</a> and the pretraining codebase I work on: <a href="https://github.com/allenai/OLMo-core">OLMo-core</a>.</span>
                        Olmo 3 model construction targets long-context reasoning, function calling, coding, instruction following, general chat, and knowledge recall. This release includes the entire model flow, i.e., the full lifecycle of the family of models, including every stage, checkpoint, data point, and dependency used to build it. Our flagship model, Olmo 3 32B Think, is the strongest fully-open thinking model released to-date.</small>
                </p>
            </div>
        </li>
    </ul>
</section>
<hr class="section-divider">
<section>
    <h2 id="projects">Projects</h2>
    <ul class="item-list">
        <li>
            <a href="https://github.com/linkedin/Liger-Kernel">Liger-Kernel</a>
            <p class="item-description">
                <small>Recently I've been contributing to Liger-Kernel, a collection of custom Triton Kernels for efficient LLM training.
                    <label for="sd-liger-kernel" class="margin-toggle sidenote-number"></label>
                    <input type="checkbox" id="sd-liger-kernel" class="margin-toggle">
                    <span class="sidenote">I've found these kernels very useful for training LLMs/VLMs on my RTX 4090.</span> My contributions, as well as those of other top collaborators, were recently featured in <a href="https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training">a post on the LinkedIn Engineering Blog</a>.</small>
            </p>
        </li>
        <li>
            <a href="https://github.com/tyler-romero/microR1">microR1</a>
            <p class="item-description">
                <small>A micro-scale DeepSeek-R1 reproduction in the style of Karpathy's <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>. Intended to be easy to understand and to hack on top of.</small>
            </p>
        </li>
    </ul>
</section>
<hr class="section-divider">
<section>
    <h2 id="reading-list">Favorite Reads</h2>
    <ul class="item-list">
        <li>
            <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a>
            <p class="item-description">
                <small>A detailed guide to large-scale training of LLMs, covering 1D through 5D training parallelism, GPU fusing and threading, and more.</small>
            </p>
        </li>
        <li>
            <a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model: A Systems View of LLMs on TPUs</a>
            <p class="item-description">
                <small>An online book that explains how TPU and GPU hardware works and how the Transformer architecture has evolved to perform well on current hardware.</small>
            </p>
        </li>
        <li>
            <a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a>
            <p class="item-description">
                <small>A great post by Horace He explaining how to speed up single-GPU training based on whether jobs are compute-, bandwidth-, or overhead-bound. See also <a href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like?</a></small>
            </p>
        </li>
        <li>
            <a href="https://siboehm.com/">sibohem</a>
            <p class="item-description">
                <small>An excellent ML engineering blog by Simon Boehm, and a large part of the inspiration for this site. I especially recommend Simon’s posts on <a href="https://siboehm.com/articles/22/Fast-MMM-on-CPU">optimizing multidimensional matrix multiplication on CPU</a> and <a href="https://siboehm.com/articles/22/pipeline-parallel-training">pipeline parallelism for distributed training</a>.</small>
            </p>
        </li>
        <li>
            <a href="https://simonwillison.net/">Simon Willison’s Weblog</a>
            <p class="item-description">
                <small>An insightful collection of links, quotes, and short blog posts that helps navigate the firehose of ML news.</small>
            </p>
        </li>
        <li>
            <a href="https://www.interconnects.ai/">Interconnects</a>
            <p class="item-description">
                <small>A substack with long-form technical posts about AI R&D by Nathan Lambert.</small>
            </p>
        </li>
        <li>
            <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">The 37 Implementation Details of Proximal Policy Optimization</a>
            <p class="item-description">
                <small>A legendary ICLR blog post diving into the (unreported/underreported) implementation details of PPO. Necessary reading for anyone working on LLM post-training with PPO or GRPO.</small>
            </p>
        </li>
        <li>
            <a href="https://maharshi.bearblog.dev/optimizing-softmax-cuda/">Learning CUDA by optimizing softmax: A worklog</a>
            <p class="item-description">
                <small>A nice post by Maharshi Pandya on optimizing a softmax CUDA kernel.</small>
            </p>
        </li>
    </ul>
</section>
<hr class="section-divider">
<section>
    <h2 id="fun">Fun Stuff</h2>
    <ul class="item-list">
        <li>
            <a href="/recipe-box">Recipe Box</a>
            <p class="item-description">
                <small>I find great joy in the process of cooking and I like to keep a recipe box of my favorite dishes.</small>
            </p>
        </li>
        <li>
            <a href="https://www.geekwire.com/2024/how-this-startup-used-ai-to-keep-raccoons-from-invading-my-house/">How this startup used AI to keep raccoons from invading my house</a>
            <p class="item-description">
                <small>My friends and I help a Seattle tech reporter keep some curious raccoons out of his living room. Related: <a href="https://www.geekwire.com/2024/found-a-raccoon-in-the-living-room-now-seeking-a-tech-solution-so-it-doesnt-happen-again/">Found a raccoon in the living room — now seeking a tech solution so it doesn’t happen again</a></small>
            </p>
        </li>
    </ul>
</section>

        </article>
        <footer>
                <hr class="slender">
    <p class="social-links">
        <a href="/" aria-label="Home"><span class="fas fa-home"></span></a>
        <a href="https://twitter.com/tyleraromero" aria-label="Twitter"><i class="fab fa-twitter-square"></i></a>
        <a href="https://github.com/tyler-romero" aria-label="GitHub"><i class="fab fa-github-square"></i></a>
        <a href="https://linkedin.com/in/tylerromero" aria-label="LinkedIn"><i class="fab fa-linkedin"></i></a>
        <a href="https://scholar.google.com/citations?user=Hmrl0FAAAAAJ" aria-label="Google Scholar"><i class="fas fa-link"></i></a>
        <a href="mailto:tyler.alexander.romero+site@gmail.com" aria-label="Email"><i class="fas fa-envelope-square"></i></a>
        <a href="/feed.xml" aria-label="RSS Feed"><i class="fas fa-rss-square"></i></a>
    </p>
    <p class="copyright">&copy; 2026 Tyler Romero</p>

        </footer>
    </body>
</html>
