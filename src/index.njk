---
layout: frontpage.njk
subtitle: Notes on Machine Learning and related topics.
tags: ["ML", "Research", "Blog", "LLMs"]
---

<section>
    <h1>Hi, I'm Tyler Romero.</h1>
    <p>
        <span class="marginnote">
            <img src="/assets/img/headshot_2.jpg"
                 alt="me"
                 class="profile-picture"
                 style="max-width: 749px;
                        height: auto" />
        </span>I am a Research Engineer at <a href="https://allenai.org/about">the Allen Institute (Ai2)</a>, where I work on <a href="https://allenai.org/language-models">open language modeling</a>.
    </p>
    <p>
        Previously, I was Lead ML Engineer at <a href="https://www.groundlight.ai/">Groundlight</a>, a startup building multimodal question-answering systems. Prior to that, I worked on large-scale recommender systems at Twitter, where I developed the ML and ran the A/B tests for the experimentally-successful yet short-lived <a href="https://techcrunch.com/2021/07/21/twitter-tests-reddit-style-upvote-and-downvote-buttons/">downvote button</a>. I also researched, trained, and shipped model architecture improvements for ranking Twitter's home timeline and conversation reply trees.
        <label for="sd-less-than-a-href" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="sd-less-than-a-href" class="margin-toggle" />
        <span class="sidenote"><a href="https://github.com/twitter/the-algorithm-ml/tree/main/projects/home/recap">And some of my work at Twitter is now open-source!</a> Although the git-blame has been sanitized.</span> Earlier in my career, I worked as a Research Scientist at Microsoft, where I built greenfield ML projects.
    </p>
    <p>
        My academic background includes a Master’s in computer science and machine learning from Stanford, and a <a href="https://engineering.tamu.edu/news/2015/10/look-college-honors-outstanding-seniors.html">Bachelor’s in computer engineering</a> from Texas A&#x26;M. As an undergraduate, I performed research on novel implementations of parallel algorithms written in C / <a href="https://cilk.mit.edu/programming/">Cilk</a> and interned as a Software Engineer at Bloomberg and Microsoft.
        <label for="sd-i-made-a-few" class="margin-toggle sidenote-number"></label>
        <input type="checkbox" id="sd-i-made-a-few" class="margin-toggle" />
        <span class="sidenote">I made a few contributions to Bloomberg’s <a href="https://www.bloomberg.com/professional/product/asset-and-investment-manager/">Asset and Investment Management</a> function and wrote Microsoft <a href="https://learn.microsoft.com/en-us/sql/machine-learning/r/ref-r-olapr">a data retrieval package for R</a> that is still supported 8 years later.</span>
    </p>
    <p>
        You can reach me via <a href="mailto:tyler.alexander.romero+site@gmail.com">email</a> or <a href="https://www.linkedin.com/in/tylerromero/">LinkedIn</a> message.
    </p>
</section>
<section>
    <h2 id="posts">Posts</h2>
    <ul>
        {% for p in collections.post | reverse %}
            <li>
                <a href="{{ p.url }}">{{ p.data.title }}</a>
                &nbsp
                <p style="margin-top: 0;">
                    <small><em>{{ p.data.date | postDate }}</em> - {{ p.data.blurb }}</small>
                </p>
            </li>
        {% endfor %}
    </ul>
</section>
<section>
    <h2 id="publications">Publications & Preprints</h2>
    <ul>
        <li>
            <a href="https://arxiv.org/abs/2512.13961"><strong>Olmo 3</strong></a> <small>Team OLMo. <em>arXiv preprint arXiv:2512.13961</em>, 2025.</small>
        </li>
    </ul>
</section>
<section>
    <h2 id="projects">Projects</h2>
    <ul>
        <li>
            <a href="https://github.com/linkedin/Liger-Kernel">Liger-Kernel</a>
            <p style="margin-top: 0;">
                <small>Recently I've been contributing to Liger-Kernel, a collection of custom Triton Kernels for efficient LLM training.
                    <label for="sd-liger-kernel" class="margin-toggle sidenote-number"></label>
                    <input type="checkbox" id="sd-liger-kernel" class="margin-toggle" />
                    <span class="sidenote">I've found these kernels very useful for training LLMs/VLMs on my RTX 4090.</span> My contributions, as well as those of other top collaborators, were recently featured in <a href="https://www.linkedin.com/blog/engineering/open-source/liger-kernel-open-source-ecosystem-for-efficient-llm-training">a post on the LinkedIn Engineering Blog</a>.</small>
            </p>
        </li>
        <li>
            <a href="https://github.com/tyler-romero/microR1">microR1</a>
            <p style="margin-top: 0;">
                <small>A micro-scale DeepSeek-R1 reproduction in the style of Karpathy's <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>. Intended to be easy to understand and to hack on top of.</small>
            </p>
        </li>
    </ul>
</section>
<section>
    <h2 id="reading-list">Favorite Reads</h2>
    <ul>
        <li>
            <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">The Ultra-Scale Playbook: Training LLMs on GPU Clusters</a>
            <p style="margin-top: 0;">
                <small>A detailed guide to large-scale training of LLMs, covering 1D through 5D training parallelism, GPU fusing and threading, and more.</small>
            </p>
        </li>
        <li>
            <a href="https://jax-ml.github.io/scaling-book/">How to Scale Your Model: A Systems View of LLMs on TPUs</a>
            <p style="margin-top: 0;">
                <small>An online book that explains how TPU and GPU hardware works and how the Transformer architecture has evolved to perform well on current hardware.</small>
            </p>
        </li>
        <li>
            <a href="https://horace.io/brrr_intro.html">Making Deep Learning Go Brrrr From First Principles</a>
            <p style="margin-top: 0;">
                <small>A great post by Horace He explaining how to speed up single-GPU training based on whether jobs are compute-, bandwidth-, or overhead-bound. See also <a href="https://www.thonking.ai/p/what-shapes-do-matrix-multiplications">What Shapes Do Matrix Multiplications Like?</a></small>
            </p>
        </li>
        <li>
            <a href="https://siboehm.com/">sibohem</a>
            <p style="margin-top: 0;">
                <small>An excellent ML engineering blog by Simon Boehm, and a large part of the inspiration for this site. I especially recommend Simon’s posts on <a href="https://siboehm.com/articles/22/Fast-MMM-on-CPU">optimizing multidimensional matrix multiplication on CPU</a> and <a href="https://siboehm.com/articles/22/pipeline-parallel-training">pipeline parallelism for distributed training</a>.</small>
            </p>
        </li>
        <li>
            <a href="https://simonwillison.net/">Simon Willison’s Weblog</a>
            <p style="margin-top: 0;">
                <small>An insightful collection of links, quotes, and short blog posts that helps navigate the firehose of ML news.</small>
            </p>
        </li>
        <li>
            <a href="https://www.interconnects.ai/">Interconnects</a>
            <p style="margin-top: 0;">
                <small>A substack with long-form technical posts about AI R&D by Nathan Lambert.</small>
            </p>
        </li>
        <li>
            <a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/">The 37 Implementation Details of Proximal Policy Optimization</a>
            <p style="margin-top: 0;">
                <small>A legendary ICLR blog post diving into the (unreported/underreported) implementation details of PPO. Necessary reading for anyone working on LLM post-training with PPO or GRPO.</small>
            </p>
        </li>
        <li>
            <a href="https://maharshi.bearblog.dev/optimizing-softmax-cuda/">Learning CUDA by optimizing softmax: A worklog</a>
            <p style="margin-top: 0;">
                <small>A nice post by Maharshi Pandya on optimizing a softmax CUDA kernel.</small>
            </p>
        </li>
    </ul>
</section>
<section>
    <h2 id="fun">Fun Stuff</h2>
    <ul>
        <li>
            <a href="/recipe-box">Recipe Box</a>
            <p style="margin-top: 0;">
                <small>I find great joy in the process of cooking and I like to keep a recipe box of my favorite dishes.</small>
            </p>
        </li>
        <li>
            <a href="https://www.geekwire.com/2024/how-this-startup-used-ai-to-keep-raccoons-from-invading-my-house/">How this startup used AI to keep raccoons from invading my house</a>
            <p style="margin-top: 0;">
                <small>My friends and I help a Seattle tech reporter keep some curious raccoons out of his living room. Related: <a href="https://www.geekwire.com/2024/found-a-raccoon-in-the-living-room-now-seeking-a-tech-solution-so-it-doesnt-happen-again/">Found a raccoon in the living room — now seeking a tech solution so it doesn’t happen again</a></small>
            </p>
        </li>
    </ul>
</section>
<section>
    <h2 id="website">Website</h2>
    <p>
        This website is made with <a href="https://www.11ty.dev/">11ty</a>, <a href="https://edwardtufte.github.io/tufte-css/">Tufte CSS</a>, and <a href="https://github.com/nerdhaus/eleventufte/tree/main">eleventufte</a>. Custom figures are made with <a href="https://excalidraw.com/">Excalidraw</a>. The combination of Tufte CSS and Excalidraw to achieve a notebook-like appearance was borrowed from <a href="https://siboehm.com/">Simon Boehm's website</a>, because having a visually appealing site helps motivate me to write.
    </p>
</section>
