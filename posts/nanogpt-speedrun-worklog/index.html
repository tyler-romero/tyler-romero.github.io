<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>NanoGPT Speedrun Living Worklog</title>
<meta name="author" content="Tyler Romero">
<meta name="description" content="How fast can I train GPT-2 on two RTX 4090 GPUs?">
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/assets/img/favicon.ico" type="image/ico">
<link rel="canonical" href="https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/">
<link type="application/atom+xml" rel="alternate" href="https://www.tylerromero.com/feed.xml" title="Tyler&#39;s Technical Blog">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff">


<!-- KaTeX Support -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<link rel="stylesheet" href="/assets/tufte.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
<!-- Goat Counter for basic view counting w/o cookies -->
<script data-goatcounter="https://tylerromero.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

        <script type="application/ld+json">
            {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": "NanoGPT Speedrun Living Worklog",
            "description": "How fast can I train GPT-2 on two RTX 4090 GPUs?",
            "author": {
                "@type": "Person",
                "name": "Tyler Romero",
                "url": "https://www.tylerromero.com"
            },
            "datePublished": "2025-01-16T08:00:00.000Z",
            "dateModified": "2025-01-17T02:13:08.136Z",
            "url": "https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.tylerromero.com/posts/nanogpt-speedrun-worklog/"
            },
            "image": "https://www.tylerromero.com",
            "keywords": "post, llm, gpt2, speedrun, nanogpt, worklog",
            "wordCount": "513",
            "articleBody": "I’ve seen some really awesome GPT-2 speedrun results from people like Keller Jordan, Fern, Braden Koszarsky, and others. I got a little inspired and wanted to see how fast I could train GPT-2 on my own hardware. Technically, the NanoGPT speedrun is to train a neural network to 3.28 validation loss on FineWeb as fast as possible on an 8xH100 machine. Keller Jordan maintains a leaderboard here. At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!). I have access to 2xRTX 4090 GPUs..."
            }
        </script>
    </head>
    <header>
        <nav class="topbar">
            <div class="right-aligned-links">
                <a href="/" class="no-tufte-underline" style="color: #246eb9;
                          margin-right: 3%">Tyler Romero</a>
                <a href="/#posts" class="no-tufte-underline">posts</a>
                <span class="desktop-only">
                    <a href="/#projects" class="no-tufte-underline">projects</a>
                    <a href="/#reading-list" class="no-tufte-underline">reading list</a>
                </span>
            </div>
        </nav>
    </header>
    <body>
        <article>
            <h1>NanoGPT Speedrun Living Worklog
</h1>
            <p class="subtitle">How fast can I train GPT-2 on two RTX 4090 GPUs?
</p>
            <p class="date">January 16, 2025</p>
            <section><p>I’ve seen <a href="https://x.com/kellerjordan0/status/1859331370268623321">some</a> <a href="https://x.com/kellerjordan0/status/1842300916864844014">really</a> <a href="https://x.com/kellerjordan0/status/1876048851158880624">awesome</a> <a href="https://x.com/hi_tysam/status/1879687807678959729">GPT-2</a> speedrun results from people like <a href="https://x.com/kellerjordan0">Keller Jordan</a>, <a href="https://x.com/hi_tysam">Fern</a>, <a href="https://x.com/KoszarskyB">Braden Koszarsky</a>, and others. I got a little inspired and wanted to see how fast I could train GPT-2 on my own hardware.</p><p>Technically, <a href="https://x.com/kellerjordan0/status/1798863559243513937">the NanoGPT speedrun</a> is to train a neural network to 3.28 validation loss on FineWeb as fast as possible on an <strong>8xH100</strong> machine. <a href="https://github.com/KellerJordan/modded-nanogpt?tab=readme-ov-file#world-record-history">Keller Jordan maintains a leaderboard here</a>. At the time of writing (Jan 16, 2025), the record is 3.14 minutes (!).</p><p>I have access to <strong>2xRTX 4090 GPUs</strong> and I want to see how fast I can train GPT-2 on them by following the same rules as the NanoGPT speedrun. If I see some success, I may try to transfer my methods to an 8xH100 node for comparison with the main leaderboard.</p><p>I’ll be documenting my progress here and updating this post as I go. Code can be found in <a href="https://github.com/tyler-romero/nanogpt-speedrun">this GitHub repo</a>.</p></section>
<section><h2 id="progress-so-far">Progress so far</h2><table>
<thead>
<tr>
<th align="left">#</th>
<th align="left">Record time</th>
<th align="left">Training Tokens</th>
<th align="left">Description</th>
<th align="left">Date</th>
<th align="left">Commit</th>
<th align="left">Log</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">1</td>
<td align="left">8.13 hours</td>
<td align="left">6.44e+09</td>
<td align="left">Initial baseline</td>
<td align="left">2025–01–16</td>
<td align="left"><a href="https://github.com/tyler-romero/nanogpt-speedrun/commit/b3c32f8937c1f4655c5eb9607970e03e351a6c08">b3c32f8</a></td>
<td align="left"><a href="https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/4c627c0d-029c-4f8a-bd18-40f99b43b22e.txt">here</a></td>
</tr>
</tbody>
</table></section>
<section><h2 id="1-initial-setup-and-baseline">1. Initial setup and baseline</h2><p>Part of the goal of this project is for me to learn as I go, so I am going to start at the beginning - with with Andrej Karpathy’s <a href="https://github.com/karpathy/llm.c/blob/7b929300217ff1a974b63791a228928b39b26409/train_gpt2.py">PyTorch GPT-2 trainer</a> from <a href="https://github.com/karpathy/llm.c">llm.c</a>. This is the script that Keller Jordan used for <a href="https://github.com/KellerJordan/modded-nanogpt/tree/master?tab=readme-ov-file#modded-nanogpt">his initial baseline</a>. This trainer is very similar to the NanoGPT trainer with some minor modifications / simplifications (such as no dropout).</p><p>I have upstreamed some QOL improvements and basic tweaks to the training script from Keller’s fork, but have not changed any of the core training / modeling logic. Specifically:</p><ol>
<li>Implemented gradient accumulation so that my 2x24GB GPUs simulate the training experience of a 8xH100 machine.</li>
<li>Increased learning rate to 0.0015 and halved the batch size (total batch size is 262144 - that is bs of <code>32/device * 2 devices * 1024 sequence length * 4 gradient accum steps</code>).</li>
<li>Improved learning rate schedule (linear warmup then linear decay).</li>
<li>Removed all affine scale/bias parameters and switched to RMSNorm.</li>
<li>Padded the vocab size from 50257 to 50304 to make it a multiple of 128 (for better tensor core utilization).</li>
</ol><p>Additionally, I added <code>wandb</code> logging for easy tracking of training progress - optimistically I may need to remove this one day as it slightly increases step time.</p><p>Commit with the initial setup is here: <a href="https://github.com/tyler-romero/nanogpt-speedrun/blob/main/logs/4c627c0d-029c-4f8a-bd18-40f99b43b22e.txt"><code>b3c32f8</code></a>.</p><p>The baseline run time on my 2xRTX 4090 setup is <strong>8.13 hours</strong>.</p><!-- TODO: plot --><!-- ## 2. Implementing major improvements from the 8xH100 leaderboard

Waiting 8 hours for a result, so I'm going to begin by implementing some of the notable improvements from the 8xH100 leaderboard. I'll start with the most impactful/easiest changes first:
1. FlexAttention (30.2% speedup)
2. Muon Optimizer (29% speedup)
3. Architectural changes (31.8% speedup, then 24% speedup)
4. Untied embeddings and lm_head (10% speedup)

### 2.1 Muon Optimizer --></section>

        </article>
        <footer>
            <footer>
    <hr class="slender">
    <p class="social-links">
        <a href="/"><span class="fas fa-home"></span></a>
        &nbsp
        <a href="https://twitter.com/tyleraromero"><i class="fab fa-twitter-square"></i></a>
        <a href="https://github.com/tyler-romero"><i class="fab fa-github-square"></i></a>
        <a href="https://linkedin.com/in/tylerromero"><i class="fab fa-linkedin"></i></a>
        <a href="mailto:tyler.alexander.romero+site@gmail.com"><i class="fas fa-envelope-square"></i></a>
        &nbsp
        <a href="/feed.xml"><i class="fas fa-rss-square"></i></a>
    </p>
    <p class="copyright" style="font-size: 1.0em;">&copy; 2025 Tyler Romero</p>
</footer>

        </footer>
    </body>
</html>
