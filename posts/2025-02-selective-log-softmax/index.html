<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax</title>
<meta name="author" content="Tyler Romero">
<meta name="description" content="Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest">
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/assets/img/favicon.ico" type="image/ico">
<link rel="canonical" href="https://www.tylerromero.com/posts/2025-02-selective-log-softmax/">
<link type="application/atom+xml" rel="alternate" href="https://www.tylerromero.com/feed.xml" title="Tyler&#39;s Technical Blog">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-line-figures/et-book-roman-line-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-display-italic-old-style-figures/et-book-display-italic-old-style-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-bold-line-figures/et-book-bold-line-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff">


<!-- KaTeX Support -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<!-- BibTeX Support -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js"></script>
<!-- Stylesheets -->
<link rel="stylesheet" href="/assets/tufte.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
<!-- Goat Counter for basic view counting w/o cookies -->
<script data-goatcounter="https://tylerromero.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

        <script type="application/ld+json">
            {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": "Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax",
            "description": "Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest",
            "author": {
                "@type": "Person",
                "name": "Tyler Romero",
                "url": "https://www.tylerromero.com"
            },
            "datePublished": "2025-02-06T08:00:00.000Z",
            "dateModified": "2025-02-07T22:42:45.554Z",
            "url": "https://www.tylerromero.com/posts/2025-02-selective-log-softmax/",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.tylerromero.com/posts/2025-02-selective-log-softmax/"
            },
            "image": "https://www.tylerromero.com",
            "keywords": "post, grpo, ppo, logprobs, logits, log-softmax, log_softmax, logsumexp, log-probabilities",
            "wordCount": "1863",
            "articleBody": "When training language models, we often need to convert logits (raw model outputs) into log probabilities. The standard approach uses log_softmax which requires computing probabilities for every token in the vocabulary at every position in the sequence. For large vocabulary sizes, this can consume significant VRAM. This is the code snippet you might seeThis sample code is not made up - it is commonly found in many popular OSS RLHF implementations, such as in TRL, Verl, and OpenRLHF.: def..."
            }
        </script>
    </head>
    <header>
        <nav class="topbar">
            <div class="right-aligned-links">
                <a href="/" class="no-tufte-underline" style="color: #246eb9;
                          margin-right: 3%">Tyler Romero</a>
                <a href="/#posts" class="no-tufte-underline">posts</a>
                <span class="desktop-only">
                    <a href="/#projects" class="no-tufte-underline">projects</a>
                    <a href="/#reading-list" class="no-tufte-underline">reading list</a>
                </span>
            </div>
        </nav>
    </header>
    <body>
        <article>
            <h1>Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax
</h1>
            <p class="subtitle">Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest
</p>
            <p class="date">February 6, 2025</p>
            <section><p>When training language models, we often need to convert logits (raw model outputs) into log probabilities. The standard approach uses <code>log_softmax</code> which requires computing probabilities for every token in the vocabulary at every position in the sequence. For large vocabulary sizes, this can consume significant VRAM. This is the code snippet you might see<label for="sd-this-sample-code-is" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sd-this-sample-code-is" class="margin-toggle"><span class="sidenote">This sample code is not made up - it is commonly found in many popular OSS RLHF implementations, such as in TRL, Verl, and OpenRLHF.</span>:</p>
<pre><code class="language-python">def naive_selective_log_softmax(logits, input_ids):
    log_probs = logits.log_softmax(dim=-1)
    return torch.gather(log_probs, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)
</code></pre>
<p>For example, with a modest vocabulary size of 32768, sequence length of 1024, and batch size of 16, computing <code>log_softmax</code> naively can consume <strong>2.1GB</strong> of VRAM! And that is in addition to the 2.1GB required to hold the logits in the first place. <strong>However, in many cases, we only need the log probabilities for specific tokens</strong> - usually the ones that were actually generated or appear in the training data.</p>
<p>This is particularly relevant when using reinforcement learning techniques like PPO and GRPO to post-train language models. These methods only require log probabilities for the tokens that were actually generated in the model’s output, not for every possible token in the vocabulary.</p>
<p>Let’s remind ourselves what <code>log_softmax</code> is actually computing for every input logit <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>:</p>
<div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mtext>softmax</mtext><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac><mo fence="true">)</mo></mrow><mspace linebreak="newline"></mspace><mo>=</mo><mi>log</mi><mo>⁡</mo><mo>(</mo><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mo>)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup><mo fence="true">)</mo></mrow><mspace linebreak="newline"></mspace><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>log</mi><mo>⁡</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow><annotation encoding="application/x-tex">\log \text{softmax}(x_i) = \log\left(\frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\right) \\
= \log(e^{x_i}) - \log\left(\sum_{j=1}^n e^{x_j}\right) \\
= x_i - \log \sum_{j=1}^n e^{x_j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.305708em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1301100000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.1637769999999996em;vertical-align:-1.4137769999999998em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div>
<p>Essentially it is just taking every individual logit and subtracting the <code>logsumexp</code> over the full logit distribution.</p>
<p> We can optimize this by:</p>
<ol>
<li>First gathering just the logits for the tokens we care about</li>
<li>Computing the softmax denominator (logsumexp) over the full logit distribution</li>
<li>Subtracting the denominator from our gathered logits to get the final log probabilities</li>
</ol>
<p>Here’s what this looks like in code:</p>
<pre><code class="language-python">def selective_log_softmax(logits, input_ids):
    token_logits = torch.gather(logits, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)

    # Compute logsumexp denominator for each sequence in the batch (loop to reduce memory peak)
    logsumexp_values = torch.stack([torch.logsumexp(l, dim=-1) for l in logits])
    token_log_probs = token_logits - logsumexp_values
    return token_log_probs
</code></pre>
<p>This approach reduces the peak memory usage by only allocating tensors that are proportional to <code>batch_size * sequence_length</code> and <code>sequence_length * vocab_size</code> rather than <code>batch_size * sequence_length * vocab_size</code>. Specifically, in this snippet, <code>torch.logsumexp(l, dim=-1)</code> internally allocates a tensor of size <code>sequence_length * vocab_size</code> in order to exponentiate the logits. And then <code>logsumexp_values</code> is a tensor of size <code>batch_size * sequence_length</code> which is much smaller than the full logits tensor.</p>
<p>Lets benchmark this approach against some alternatives:</p>
<pre><code class="language-python">import time
import torch

def naive_method(logits, input_ids):
    log_probs = logits.log_softmax(dim=-1)  # (bs, seq_len, vocab_size)
    return torch.gather(log_probs, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)

def method_1(logits, input_ids):  # compute log_softmax in a loop to reduce peak memory
    per_token_logps = []
    for logits_row, input_ids_row in zip(logits, input_ids):
        log_probs = logits_row.log_softmax(dim=-1)  # (seq_len, vocab_size)
        token_log_prob = torch.gather(log_probs, dim=-1, index=input_ids_row.unsqueeze(-1)).squeeze(-1)
        per_token_logps.append(token_log_prob)
    return torch.stack(per_token_logps)

def method_2(logits, input_ids):  # avoid materializing unneeded log_probs to reduce peak memory
    token_logits = torch.gather(logits, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)
    logsumexp_values = torch.logsumexp(logits, dim=-1)
    token_log_probs = token_logits - logsumexp_values  # log_softmax(logits) = logits - log(sum(exp(logits)))
    return token_log_probs

def method_3(logits, input_ids):  # combine methods 1 and 2
    per_token_logps = []
    for logits_row, input_ids_row in zip(logits, input_ids):
        token_logits = torch.gather(logits_row, dim=-1, index=input_ids_row.unsqueeze(-1)).squeeze(-1)
        token_log_prob = token_logits - torch.logsumexp(logits_row, dim=-1)
        per_token_logps.append(token_log_prob)
    return torch.stack(per_token_logps)

def efficient_method(logits, input_ids):  # pull everything out of the loop except logsumexp
    token_logits = torch.gather(logits, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)
    logsumexp_values = torch.stack([torch.logsumexp(l, dim=-1) for l in logits])
    token_log_probs = token_logits - logsumexp_values
    return token_log_probs

def measure_memory_and_time(func, logits, input_ids):
    torch.cuda.reset_peak_memory_stats()
    start_time = time.perf_counter()
    result = func(logits, input_ids)
    end_time = time.perf_counter()
    mem_peak = torch.cuda.max_memory_allocated()
    return result, end_time - start_time, mem_peak

# Simulated data
torch.manual_seed(42)
vocab_size = 32768
seq_len = 1024
batch_size = 16

device = "cuda" if torch.cuda.is_available() else "cpu"
logits = torch.randn(batch_size, seq_len, vocab_size, device=device, dtype=torch.float32)
input_ids = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
logit_mem = torch.cuda.max_memory_allocated()

# Run all methods
naive_result, naive_time, naive_mem = measure_memory_and_time(naive_method, logits, input_ids)
method1_result, method1_time, method1_mem = measure_memory_and_time(method_1, logits, input_ids)
method2_result, method2_time, method2_mem = measure_memory_and_time(method_2, logits, input_ids)
method3_result, method3_time, method3_mem = measure_memory_and_time(method_3, logits, input_ids)
efficient_result, efficient_time, efficient_mem = measure_memory_and_time(efficient_method, logits, input_ids)

# Check equivalence
print("Logits Dtype:", logits.dtype)
print("Max absolute difference (naive and 1):", (naive_result - method1_result).abs().max().item())
print("Max absolute difference (naive and 2):", (naive_result - method2_result).abs().max().item())
print("Max absolute difference (naive and 3):", (naive_result - method3_result).abs().max().item())
print("Max absolute difference (naive and efficient):", (naive_result - efficient_result).abs().max().item())
print("Memory consumed by logits: {:.2f} MB".format(logit_mem / 1e6))
print("Naive method time:      {:.6f} sec, Memory peak: {:.2f} MB".format(naive_time, naive_mem / 1e6))
print("Method 1 time:          {:.6f} sec, Memory peak: {:.2f} MB".format(method1_time, method1_mem / 1e6))
print("Method 2 time:          {:.6f} sec, Memory peak: {:.2f} MB".format(method2_time, method2_mem / 1e6))
print("Method 3 time:          {:.6f} sec, Memory peak: {:.2f} MB".format(method3_time, method3_mem / 1e6))
print("Efficient method time:  {:.6f} sec, Memory peak: {:.2f} MB".format(efficient_time, efficient_mem / 1e6))

# Results:
# Logits Dtype: torch.float32
# Memory consumed by logits: 2147.61 MB
# Max absolute difference (naive and 1): 0.0
# Max absolute difference (naive and 2): 1.9073486328125e-06
# Max absolute difference (naive and 3): 1.9073486328125e-06
# Max absolute difference (naive and efficient): 1.9073486328125e-06
# Naive method time:      0.036307 sec, Memory peak: 4295.16 MB
# Method 1 time:          0.012156 sec, Memory peak: 2416.18 MB
# Method 2 time:          0.134651 sec, Memory peak: 4295.43 MB
# Method 3 time:          0.001496 sec, Memory peak: 2282.10 MB
# Efficient method time:  0.000918 sec, Memory peak: 2282.23 MB
</code></pre>
<p>In this benchmark setting, <strong>peak VRAM usage for this operation was reduced by 47% (from 4295MB to 2282MB)</strong> while maintaining numerical stability. And <strong>most of the memory consumed now is due to the size of the input logits (2147MB)</strong>. Additionally, the proposed method is about 40x faster than the naive implementation (0.0363s vs 0.0009s). Although, in practice, the speed of this operation is not very consequential.</p>
<p>It is important to note that the <code>efficient_method</code> is not numerically stable when logits are cast to <code>bfloat16</code> or <code>float16</code>:</p>
<pre><code class="language-python"># Results:
# Logits Dtype: torch.bfloat16
# Memory consumed by logits: 1073.87 MB
# Max absolute difference (naive and 1): 0.0
# Max absolute difference (naive and 2): 0.0625
# Max absolute difference (naive and 3): 0.0625
# Max absolute difference (naive and efficient): 0.0625    # &#x3C;-- this is the issue
# Naive method time:      0.027738 sec, Memory peak: 2147.65 MB
# Method 1 time:          0.003902 sec, Memory peak: 1208.15 MB
# Method 2 time:          0.101947 sec, Memory peak: 2147.78 MB
# Method 3 time:          0.001430 sec, Memory peak: 1141.12 MB
# Efficient method time:  0.000880 sec, Memory peak: 1141.18 MB
</code></pre>
<p>So it makes sense to use the <code>efficient_method</code> for <code>torch.float32</code> and <code>torch.float64</code> and <code>method_1</code> for <code>torch.bfloat16</code> and <code>torch.float16</code>. The complete code snippet is as follows:</p>
<pre><code class="language-python">def selective_log_softmax(logits, input_ids):
    """Compute log softmax probabilities for selected tokens.

    For float32/float64 tensors, uses a fast and memory-efficient implementation.
    For half-precision (float16/bfloat16), uses a numerically stable implementation,
        which is slightly less memory-efficent than the best implementation for float32/float64,
        but still much more memory-efficient and faster than thee naive implementation.

    Args:
        logits (torch.Tensor): Logits tensor of shape (batch_size, sequence_length, vocab_size)
        input_ids (torch.Tensor): Token indices of shape (batch_size, sequence_length)

    Returns:
        torch.Tensor: Log probabilities for the selected tokens
    """
    if logits.dtype in (torch.float32, torch.float64):
        token_logits = torch.gather(logits, dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)
        logsumexp_values = torch.stack([torch.logsumexp(l, dim=-1) for l in logits])
        token_log_probs = token_logits - logsumexp_values

    else:  # for half-precision, use numerically stable method
        token_log_probs = []
        for logits_row, input_ids_row in zip(logits, input_ids):
            log_probs_row = logits_row.log_softmax(dim=-1)
            token_log_probs_row = torch.gather(log_probs_row, dim=-1, index=input_ids_row.unsqueeze(-1)).squeeze(-1)
            token_log_probs.append(token_log_probs_row)
        token_log_probs = torch.stack(token_log_probs)

    return token_log_probs
</code></pre>
<p>I have contributed this optimization to several popular RLHF libraries, including <a href="https://github.com/huggingface/trl">TRL</a> [<a href="https://github.com/huggingface/trl/pull/2773">PR 1</a>, <a href="https://github.com/huggingface/trl/pull/2799">PR 2</a>], <a href="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF</a> [<a href="https://github.com/OpenRLHF/OpenRLHF/pull/718">PR 3</a>], and <a href="https://github.com/volcengine/verl">Verl</a> [<a href="https://github.com/volcengine/verl/pull/220">PR 4</a>].</p>
<p>Thanks to <a href="https://github.com/qgallouedec">Quentin Gallouédec</a> for providing the benchmarking script and for suggesting to pull the <code>gather</code> and element-wise subtraction operations out of the for loop in order to improve operation speed (<code>method_3</code> → <code>efficient_method</code>).</p>
</section>
        </article>
        <footer>
            <footer>
    <hr class="slender">
    <p class="social-links">
        <a href="/"><span class="fas fa-home"></span></a>
        &nbsp
        <a href="https://twitter.com/tyleraromero"><i class="fab fa-twitter-square"></i></a>
        <a href="https://github.com/tyler-romero"><i class="fab fa-github-square"></i></a>
        <a href="https://linkedin.com/in/tylerromero"><i class="fab fa-linkedin"></i></a>
        <a href="mailto:tyler.alexander.romero+site@gmail.com"><i class="fas fa-envelope-square"></i></a>
        &nbsp
        <a href="/feed.xml"><i class="fas fa-rss-square"></i></a>
    </p>
    <p class="copyright" style="font-size: 1.0em;">&copy; 2025 Tyler Romero</p>
</footer>

        </footer>
    </body>
</html>
