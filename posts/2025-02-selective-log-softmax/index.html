<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax</title>
<meta name="author" content="Tyler Romero">
<meta name="description" content="Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest">
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/assets/img/favicon.ico" type="image/ico">
<link rel="canonical" href="https://www.tylerromero.com/posts/2025-02-selective-log-softmax/">
<link type="application/atom+xml" rel="alternate" href="https://www.tylerromero.com/feed.xml" title="Tyler&#39;s Technical Blog">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,300..700;1,300..700&display=swap">
<link rel="preload" as="font" type="font/woff" crossorigin="" href="/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/fonts/Virgil.woff2">
<!-- Open Graph -->
<meta property="og:title" content="Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax">
<meta property="og:description" content="Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest">
<meta property="og:url" content="https://www.tylerromero.com/posts/2025-02-selective-log-softmax/">
<meta property="og:type" content="article">
<meta property="og:site_name" content="Tyler&#39;s Technical Blog">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax">
<meta name="twitter:description" content="Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest">




<!-- KaTeX Support -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>



<!-- Prism.js Syntax Highlighting -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css">
<script defer="" src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-python.min.js"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-bash.min.js"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-css.min.js"></script>

<!-- Rough Notation — hand-drawn annotations -->
<script defer="" src="https://unpkg.com/rough-notation/lib/rough-notation.iife.js"></script>
<!-- Stylesheets -->
<link rel="stylesheet" href="/assets/tufte.min.css">
<!-- Goat Counter for basic view counting w/o cookies -->
<script data-goatcounter="https://tylerromero.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

        
        <script type="application/ld+json">
            {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": "Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax",
            "description": "Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest",
            "author": {
                "@type": "Person",
                "name": "Tyler Romero",
                "url": "https://www.tylerromero.com"
            },
            "datePublished": "2025-02-06T08:00:00.000Z",
            "dateModified": "2026-02-18T23:00:24.990Z",
            "url": "https://www.tylerromero.com/posts/2025-02-selective-log-softmax/",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.tylerromero.com/posts/2025-02-selective-log-softmax/"
            },
            "image": "https://www.tylerromero.com",
            "keywords": "post, grpo, ppo, logprobs, logits, log-softmax, log_softmax, logsumexp, log-probabilities",
            "wordCount": "2620",
            "articleBody": "When training language models, we often need to convert logits (raw model outputs) into log probabilities. The standard approach uses log_softmax which requires computing probabilities for every token in the vocabulary at every position in the sequence. For large vocabulary sizes, this can consume significant VRAMVRAM is a GPU’s fast, onboard memory. VRAM is the main bottleneck to training larger models on a fixed number of GPUs. It is also a bottleneck on batch size, which affects training..."
            }
        </script>

    </head>
    <body>
        <header>
            <nav class="topbar">
                <div class="right-aligned-links">
                    <a href="/" class="no-tufte-underline nav-brand">Tyler Romero</a>
                    
                    <a href="/#posts" class="no-tufte-underline">posts</a>
                    <span class="desktop-only">
                        <a href="/#projects" class="no-tufte-underline">projects</a>
                        <a href="/#reading-list" class="no-tufte-underline">reading list</a>
                    </span>
                    
                </div>
            </nav>
        </header>
        <script>
            document.addEventListener('DOMContentLoaded', function() {
                if (typeof RoughNotation === 'undefined') return;
                var links = document.querySelectorAll('.topbar a:not(.nav-brand)');
                links.forEach(function(link) {
                    var annotation = RoughNotation.annotate(link, {
                        type: 'underline',
                        color: getComputedStyle(document.documentElement).getPropertyValue('--link-color').trim(),
                        strokeWidth: 2,
                        animationDuration: 300,
                        iterations: 1
                    });
                    link.addEventListener('mouseenter', function() { annotation.show(); });
                    link.addEventListener('mouseleave', function() { annotation.hide(); });
                });
            });
        </script>
        <article>
            
            <div class="progress-bar"></div>
            <h1>Reducing VRAM Footprint in PPO and GRPO Using Selective Log-Softmax
</h1>
            <p class="subtitle">Slash VRAM usage by half when computing log probs by selectively applying log-softmax only to tokens of interest
</p>
            <p class="date">February 6, 2025</p>
            <nav class="toc"></nav>
            <script>
                (function() {
                    var bar = document.querySelector('.progress-bar');
                    window.addEventListener('scroll', function() {
                        var scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
                        var scrollHeight = document.documentElement.scrollHeight - document.documentElement.clientHeight;
                        var progress = scrollHeight > 0 ? (scrollTop / scrollHeight) * 100 : 0;
                        bar.style.width = progress + '%';
                    });

                    var headings = document.querySelectorAll('article h2, article h3');
                    if (headings.length >= 3) {
                        var toc = document.querySelector('.toc');
                        var ol = document.createElement('ol');
                        var currentH2Item = null;
                        var nestedOl = null;
                        headings.forEach(function(h) {
                            var li = document.createElement('li');
                            var a = document.createElement('a');
                            a.href = '#' + h.id;
                            a.textContent = h.textContent;
                            li.appendChild(a);
                            if (h.tagName === 'H2') {
                                currentH2Item = li;
                                nestedOl = null;
                                ol.appendChild(li);
                            } else {
                                if (!nestedOl) {
                                    nestedOl = document.createElement('ol');
                                    if (currentH2Item) {
                                        currentH2Item.appendChild(nestedOl);
                                    } else {
                                        ol.appendChild(li);
                                        return;
                                    }
                                }
                                nestedOl.appendChild(li);
                            }
                        });
                        toc.appendChild(ol);
                    }
                })();
            </script>
            <script>
                document.addEventListener('DOMContentLoaded', function() {
                    if (typeof RoughNotation === 'undefined') return;
                    var reduceMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;
                    var headings = document.querySelectorAll('article h2');
                    var observer = new IntersectionObserver(function(entries) {
                        entries.forEach(function(entry) {
                            if (entry.isIntersecting) {
                                var annotation = RoughNotation.annotate(entry.target, {
                                    type: 'bracket',
                                    brackets: ['left'],
                                    color: 'rgba(86, 130, 89, 0.5)',
                                    strokeWidth: 2,
                                    padding: [2, 0, 2, 8],
                                    animationDuration: 600,
                                    animate: !reduceMotion
                                });
                                annotation.show();
                                observer.unobserve(entry.target);
                            }
                        });
                    }, { threshold: 0.5 });
                    headings.forEach(function(h) { observer.observe(h); });
                });
            </script>

            <section><p>When training language models, we often need to convert logits (raw model outputs) into log probabilities. The standard approach uses <code>log_softmax</code> which requires computing probabilities for every token in the vocabulary at every position in the sequence. For large vocabulary sizes, this can consume significant VRAM<label for="sd-vram-is-a-gpu-s-fast" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sd-vram-is-a-gpu-s-fast" class="margin-toggle"><span class="sidenote">VRAM is a GPU’s fast, onboard memory. VRAM is the main bottleneck to training larger models on a fixed number of GPUs. It is also a bottleneck on batch size, which affects training throughput and stability.</span>. This is the code you might see:</p>
<pre><code class="language-python">def naive_selective_log_softmax(logits, index):
    logprobs = logits.log_softmax(dim=-1)  # shape: (batch_size, seq_len, vocab_size)
    return torch.gather(logprobs, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
</code></pre>
<p>For example, with a modest vocabulary size of 32768, sequence length of 1024, and batch size of 16, computing <code>log_softmax</code> naively can consume <strong>2.1GB</strong> of VRAM! And that is in addition to the 2.1GB required to hold the logits in the first place. <strong>However, in many cases, we only need the log probabilities for specific tokens</strong> - usually the ones that were actually generated or appear in the training data.</p>
<p>This optimization is especially valuable for reinforcement learning algorithms like PPO and GRPO that fine-tune language models. These methods only require log probabilities for the tokens that were actually generated in the model’s output, not for every possible token in the vocabulary. Additionally, for a typical implementation of one of these algorithms, <strong>peak VRAM consumption occurs from materializing these log probabilities!</strong> So optimizing<label for="sd-less-than-a-href" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sd-less-than-a-href" class="margin-toggle"><span class="sidenote"><a href="#efficient-solution">To jump to the optimized solution, click here</a>.</span> this operation can directly allow us to train with a larger batch size.</p>
<p>Let’s remind ourselves what <code>log_softmax</code> is actually computing for every input logit <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></span>:</p>
<div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mtext>softmax</mtext><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow></mfrac><mo fence="true">)</mo></mrow><mspace linebreak="newline"></mspace><mo>=</mo><mi>log</mi><mo>⁡</mo><mo>(</mo><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mo>)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup><mo fence="true">)</mo></mrow><mspace linebreak="newline"></mspace><mo>=</mo><msub><mi>x</mi><mi>i</mi></msub><mo>−</mo><mi>log</mi><mo>⁡</mo><munderover><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mi>e</mi><msub><mi>x</mi><mi>j</mi></msub></msup></mrow><annotation encoding="application/x-tex">\log \text{softmax}(x_i) = \log\left(\frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}\right) \\
= \log(e^{x_i}) - \log\left(\sum_{j=1}^n e^{x_j}\right) \\
= x_i - \log \sum_{j=1}^n e^{x_j}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:3.0000299999999998em;vertical-align:-1.25003em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.341392em;"><span style="top:-2.305708em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.804292em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6064620000000001em;"><span style="top:-3.0050700000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1301100000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.1637769999999996em;vertical-align:-1.4137769999999998em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size4">(</span></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size4">)</span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:3.0651740000000007em;vertical-align:-1.4137769999999998em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6513970000000007em;"><span style="top:-1.872331em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.050005em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.3000050000000005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.4137769999999998em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2818857142857143em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></div>
<p>Essentially it is just taking every individual logit and subtracting the <a href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch-logsumexp"><code>logsumexp</code></a> over the full logit distribution.</p>
<p>We can optimize this by:</p>
<ol>
<li>Computing the <code>logsumexp</code> values over the full logit distribution</li>
<li>Gathering just the logits for the tokens we care about</li>
<li>Subtracting the <code>logsumexp</code> values from our gathered logits to get the final log probabilities</li>
</ol>
<p>Here’s what this looks like in code:</p>
<pre><code class="language-python">def selective_log_softmax_take1(logits, index):
    logsumexp_values = torch.logsumexp(logits, dim=-1)  # shape: (batch_size, seq_len)
    token_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)  # shape: (batch_size, seq_len)
    token_logprobs = token_logits - logsumexp_values  # shape: (batch_size, seq_len)
    return token_logprobs
</code></pre>
<p>On the surface, it looks like this should decrease the memory requirements of the selective log-softmax operation — we are now only outputting tensors of size <code>batch_size * sequence_length</code> rather than <code>batch_size * sequence_length * vocab_size</code>. However, there is a catch. Internally, <code>torch.logsumexp()</code> allocates a tensor of size <code>batch_size * sequence_length * vocab_size</code> in order to exponentiate the logits. So, unfortunately, our peak memory consumption has not decreased at all.</p>
<p>What can we do to improve this situation?</p>
<p>Well, we could just compute the logsumexp values one-by-one for each sequence in the batch. That would mean that <code>torch.logsumexp()</code> only materializes a <code>sequence_length * vocab_size</code> tensor internally.</p>
<pre><code class="language-python">def selective_log_softmax_take2(logits, index):
    logsumexp_values = torch.stack([torch.logsumexp(l, dim=-1) for l in logits])
    token_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
    token_logprobs = token_logits - logsumexp_values
    return token_logprobs
</code></pre>
<p>This approach should effectively reduce peak memory usage by only allocating tensors that are proportional to <code>batch_size * sequence_length</code> and <code>sequence_length * vocab_size</code> rather than <code>batch_size * sequence_length * vocab_size</code>.</p>
<p>Let’s run a benchmark to see if we are correct. We’ll also include the following ablation that simply computes <code>log_softmax</code> in a loop over the batch dimension.</p>
<pre><code class="language-python">def selective_log_softmax_ablation1(logits, index):
    token_logprobs = []
    for logits_row, index_row in zip(logits, index):
        logprobs_row = logits_row.log_softmax(dim=-1)  # (seq_len, vocab_size)
        token_logprobs_row = torch.gather(logprobs_row, dim=-1, index=index_row.unsqueeze(-1)).squeeze(-1)
        token_logprobs.append(token_logprobs_row)
    return torch.stack(token_logprobs)
</code></pre>
<p>Here is the benchmark script:</p>
<pre><code class="language-python">import time
import torch

def measure_memory_and_time(func, logits, index, n_runs=100):
    torch.cuda.reset_peak_memory_stats()
    result = func(logits, index)
    mem_peak = torch.cuda.max_memory_allocated()
    start_time = time.perf_counter()
    for _ in range(n_runs):
        func(logits, index)
    avg_time = (time.perf_counter() - start_time) / n_runs
    return result, avg_time, mem_peak

# Simulated data
torch.manual_seed(42)
vocab_size = 32768
seq_len = 1024
batch_size = 16

device = "cuda" if torch.cuda.is_available() else "cpu"
logits = torch.randn(batch_size, seq_len, vocab_size, device=device, dtype=torch.float32)
index = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)
logit_mem = torch.cuda.max_memory_allocated()

# Run all methods
naive_result, naive_time, naive_mem = measure_memory_and_time(naive_selective_log_softmax, logits, index)
take1_result, take1_time, take1_mem = measure_memory_and_time(selective_log_softmax_take1, logits, index)
take2_result, take2_time, take2_mem = measure_memory_and_time(selective_log_softmax_take2, logits, index)
ablation1_result, ablation1_time, ablation1_mem = measure_memory_and_time(selective_log_softmax_ablation1, logits, index)

# Check equivalence
print("Logits Dtype:", logits.dtype)
print("Max absolute difference (naive and take1):", (naive_result - take1_result).abs().max().item())
print("Max absolute difference (naive and take2):", (naive_result - take2_result).abs().max().item())
print("Max absolute difference (naive and ablation1):", (naive_result - ablation1_result).abs().max().item())
print("Memory consumed by logits: {:.2f} MB".format(logit_mem / 1e6))
print("Naive method time:      {:.6f} sec, Memory peak: {:.2f} MB".format(naive_time, naive_mem / 1e6))
print("Take1 method time:      {:.6f} sec, Memory peak: {:.2f} MB".format(take1_time, take1_mem / 1e6))
print("Take2 method time:      {:.6f} sec, Memory peak: {:.2f} MB".format(take2_time, take2_mem / 1e6))
print("Ablation1 method time:  {:.6f} sec, Memory peak: {:.2f} MB".format(ablation1_time, ablation1_mem / 1e6))
</code></pre>
<p>Running this benchmark<label for="md-memory-usage-vs" class="margin-toggle ">&#8853;</label><input type="checkbox" id="md-memory-usage-vs" class="margin-toggle"><span class="marginnote">Memory usage vs. vocabulary size. <code>take1</code> is obscured by <code>naive</code> because they have the same memory requirements. <picture><source type="image/webp" srcset="/assets/img/5CQHH5jD6T-300.webp 300w, /assets/img/5CQHH5jD6T-600.webp 600w, /assets/img/5CQHH5jD6T-900.webp 900w, /assets/img/5CQHH5jD6T-2400.webp 2400w" sizes="(max-width: 900px) 100vw, 900px"><img loading="lazy" decoding="async" class="responsive-image" src="/assets/img/5CQHH5jD6T-300.png" alt="Memory usage vs vocabulary size for different selective log softmax implementations" width="2400" height="1800" srcset="/assets/img/5CQHH5jD6T-300.png 300w, /assets/img/5CQHH5jD6T-600.png 600w, /assets/img/5CQHH5jD6T-900.png 900w, /assets/img/5CQHH5jD6T-2400.png 2400w" sizes="(max-width: 900px) 100vw, 900px"></picture></span> script with logits stored in <code>float32</code> gives the following output:</p>
<pre><code class="language-text">Logits Dtype: torch.float32
Max absolute difference (naive and take1): 1.9073486328125e-06
Max absolute difference (naive and take2): 1.9073486328125e-06
Max absolute difference (naive and ablation1): 0.0
Memory consumed by logits: 2147.61 MB
Naive method time:      0.000018 sec, Memory peak: 4295.16 MB
Take1 method time:      0.000965 sec, Memory peak: 4295.29 MB
Take2 method time:      0.012608 sec, Memory peak: 2282.03 MB
Ablation1 method time:  0.004153 sec, Memory peak: 2416.31 MB
</code></pre>
<p>In this benchmark setting, <strong>peak VRAM usage for this operation was reduced by 47% (from 4295MB to 2282MB)</strong> while maintaining numerical stability. And <strong>most of the memory consumed now is due to the size of the input logits (2147MB)</strong>. The proposed method is notably slower than the naive method, although, in practice (for LLM post-training), the speed of this operation is not very consequential.</p>
<h3 id="ablation-analysis">Ablation Analysis</h3>
<p>One might note that the ablation method also only allocates tensors proportional to <code>sequence_length * vocab_size</code>, so why does it consume more memory than <code>selective_log_softmax_take2</code>? This is because of the gradient formulas for <code>log_softmax()</code> and <code>logsumexp()</code> require different intermediate values to be stored for the backward computation.</p>
<p>For <code>log_softmax</code>, the gradient formula is:</p>
<div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mtext>log_softmax</mtext><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>j</mi></msub></mrow></mfrac><mo>=</mo><msub><mi>δ</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>−</mo><mtext>softmax</mtext><mo>(</mo><msub><mi>x</mi><mi>j</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\frac{\partial \text{log\_softmax}(x_i)}{\partial x_j} = \delta_{ij} - \text{softmax}(x_j)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.4221079999999997em;vertical-align:-0.972108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.6999999999999997em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord text"><span class="mord">log_softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.972108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.980548em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03785em;">δ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:-0.03785em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div>
<p>For <code>logsumexp</code>, the gradient formula is:</p>
<div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mtext>logsumexp</mtext><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mtext>softmax</mtext><mo>(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>)</mo></mrow><annotation encoding="application/x-tex">\frac{\partial \text{logsumexp}(x)}{\partial x_i} = \text{softmax}(x_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.263em;vertical-align:-0.8360000000000001em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.3139999999999996em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord" style="margin-right:0.05556em;">∂</span><span class="mord text"><span class="mord">logsumexp</span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8360000000000001em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord text"><span class="mord">softmax</span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span></div>
<p>For the backward pass through <code>logsumexp</code>, we need:</p>
<ul>
<li>Softmax of input: <code>(sequence_length, vocab_size)</code></li>
</ul>
<p>For the backward pass through <code>log_softmax</code>, we need:</p>
<ul>
<li>Softmax of input: <code>(sequence_length, vocab_size)</code></li>
<li>Original <code>log_softmax</code> output: <code>(sequence_length, vocab_size)</code></li>
</ul>
<p>So while both methods avoid allocating additional <code>vocab_size</code>-scale tensors during the forward pass, <code>selective_log_softmax_ablation1</code> needs to store the full <code>log_softmax</code> output for the backward pass, leading to higher memory usage.</p>
<h3 id="numerical-stability">Numerical Stability</h3>
<p>It is important to note that the <code>selective_log_softmax_take2</code> is not numerically stable when logits are cast to <code>bfloat16</code> or <code>float16</code>:</p>
<pre><code class="language-text">Logits Dtype: torch.bfloat16
Max absolute difference (naive and take1): 0.0625
Max absolute difference (naive and take2): 0.0625  # &#x3C;- this is the issue
Max absolute difference (naive and ablation1): 0.0
Memory consumed by logits: 1073.87 MB
Naive method time:      0.000018 sec, Memory peak: 2147.65 MB
Take1 method time:      0.000474 sec, Memory peak: 2147.75 MB
Take2 method time:      0.005142 sec, Memory peak: 1141.11 MB
Ablation1 method time:  0.002016 sec, Memory peak: 1208.22 MB
</code></pre>
<p>Therefore, we should use <code>selective_log_softmax_take2</code> when working with full precision (<code>torch.float32</code> and <code>torch.float64</code>) tensors, and fall back to <code>selective_log_softmax_ablation1</code> when using reduced precision (<code>torch.bfloat16</code> and <code>torch.float16</code>) tensors to maintain accuracy.</p>
<h3 id="efficient-solution">Efficient Solution</h3>
<p>The complete code snippet is as follows:</p>
<pre><code class="language-python">def selective_log_softmax(logits, index):
    """Compute log softmax probabilities for selected tokens.

    Args:
        logits (`torch.Tensor`):
            Logits tensor of shape `(..., num_classes)`.
        index (`torch.Tensor`):
            Index tensor of shape `(...)`, specifying the positions to gather from the log-softmax output.
    Returns:
        `torch.Tensor`:
            Gathered log probabilities with the same shape as `index`.
    """
    if logits.dtype in [torch.float32, torch.float64]:
        logsumexp_values = torch.stack([torch.logsumexp(lg, dim=-1) for lg in logits])  # loop to reduce peak mem consumption
        selected_logits = torch.gather(logits, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
        token_logprobs = selected_logits - logsumexp_values  # log_softmax(x_i) = x_i - logsumexp(x)
    else:
        # logsumexp approach is unstable with bfloat16, fall back to slightly less efficent approach
        token_logprobs = []
        for logits_row, index_row in zip(logits, index):  # loop to reduce peak mem consumption
            logprobs_row = logits_row.log_softmax(dim=-1)
            token_logprobs_row = torch.gather(logprobs_row, dim=-1, index=index_row.unsqueeze(-1)).squeeze(-1)
            token_logprobs.append(token_logprobs_row)
        token_logprobs = torch.stack(token_logprobs)
    return token_logprobs
</code></pre>
<p>I have contributed this optimization to several popular open-source RLHF libraries, including <a href="https://github.com/huggingface/trl">huggingface/TRL</a> [<a href="https://github.com/huggingface/trl/pull/2773">PR 1</a>, <a href="https://github.com/huggingface/trl/pull/2799">PR 2</a>], <a href="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF</a> [<a href="https://github.com/OpenRLHF/OpenRLHF/pull/718">PR 3</a>], <a href="https://github.com/volcengine/verl">Verl</a> [<a href="https://github.com/volcengine/verl/pull/220">PR 4</a>], and <a href="https://github.com/allenai/open-instruct">allenai/open-instruct</a> [<a href="https://github.com/allenai/open-instruct/pull/584">PR 5</a>]. This implementation was also incorporated into <a href="https://github.com/PrimeIntellect-ai/prime-rl">PrimeIntellect-ai/prime-rl</a> [<a href="https://github.com/PrimeIntellect-ai/prime-rl/blob/a092a54029549d600d32d3b3f123ea3607498604/src/zeroband/training/loss.py#L47">here</a>] for the training of the <a href="https://storage.googleapis.com/public-technical-paper/INTELLECT_2_Technical_Report.pdf">INTELLECT-2</a> model.</p>
<p>Here is the actual GPU memory usage on an RTX 4090 (24GB VRAM) before and after implementing selective log-softmax in TRL’s <code>GRPOTrainer</code>: <picture><source type="image/webp" srcset="/assets/img/C4oDMth8kH-300.webp 300w, /assets/img/C4oDMth8kH-600.webp 600w, /assets/img/C4oDMth8kH-900.webp 900w, /assets/img/C4oDMth8kH-1922.webp 1922w" sizes="(max-width: 900px) 100vw, 900px"><img loading="lazy" decoding="async" class="responsive-image" src="/assets/img/C4oDMth8kH-300.png" alt="Memory usage reduction from selective log-softmax in TRL" width="1922" height="1310" srcset="/assets/img/C4oDMth8kH-300.png 300w, /assets/img/C4oDMth8kH-600.png 600w, /assets/img/C4oDMth8kH-900.png 900w, /assets/img/C4oDMth8kH-1922.png 1922w" sizes="(max-width: 900px) 100vw, 900px"></picture></p>
<p>A 10% reduction in peak VRAM requirements is a great improvement for such a simple change!</p>
<h3 id="a-note-on-torchcompile">A note on <code>torch.compile</code></h3>
<p>When using <code>torch.compile()</code>, PyTorch will attempt to fuse operations and generate optimized CUDA kernels using <a href="https://github.com/openai/triton">Triton</a>. For our selective log-softmax implementation, this means PyTorch may be able to take the naive implementation and fuse the <code>log_softmax</code> and <code>gather</code> operations into a single kernel, potentially reducing memory consumption.</p>
<pre><code class="language-python">@torch.compile(dynamic=True)
def compiled_selective_log_softmax(logits, index):
    logprobs = logits.log_softmax(dim=-1)
    return torch.gather(logprobs, dim=-1, index=index.unsqueeze(-1)).squeeze(-1)
</code></pre>
<p>If we benchmark this method using <code>torch==2.6.0</code> and <code>triton==3.2.0</code>, we see these results when logits are in <code>float32</code>:</p>
<pre><code class="language-text">Max absolute difference (naive and compiled): 9.5367431640625e-07
Compiled method time:  0.000073 sec, Memory peak: 2147.94 MB
</code></pre>
<p>And these results when logits are in <code>bfloat16</code><label for="sd-interestingly-less" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sd-interestingly-less" class="margin-toggle"><span class="sidenote">Interestingly, <code>torch.compile</code> generates a kernel that maintains exact numerical equivalence for half-precision dtypes.</span>:</p>
<pre><code class="language-text">Max absolute difference (naive and compiled): 0.0
Compiled method time:  0.000129 sec, Memory peak: 1074.04 MB
</code></pre>
<p>Very impressive! This is both faster and more memory efficient than our hand-rolled solution, while being numerically stable. And the <code>dynamic=True</code> flag means that we shouldn’t need to recompile every time a new sequence length is used.</p>
<p>The only reason not to use this method is if you are in a setting where <code>torch.compile</code> usage is supposed to be enabled/disabled via a user-passed flag. Which is the case for most open-source libraries that use <code>torch</code>. For your own projects, the compiled version is recommended!</p>
<hr>
<p>Thanks to <a href="https://github.com/qgallouedec">Quentin Gallouédec</a> for providing the initial benchmarking script and suggesting to pull <code>gather</code> out of the loop over <code>logsumexp</code> to improve performance.</p>
</section>
        </article>
        <footer>
                <hr class="slender">
    <p class="social-links">
        <a href="/" aria-label="Home"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 3L4 9v12h5v-7h6v7h5V9z"></path></svg></a>
        <a href="https://twitter.com/tyleraromero" aria-label="Twitter"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M18.244 2.25h3.308l-7.227 8.26 8.502 11.24H16.17l-5.214-6.817L4.99 21.75H1.68l7.73-8.835L1.254 2.25H8.08l4.713 6.231zm-1.161 17.52h1.833L7.084 4.126H5.117z"></path></svg></a>
        <a href="https://github.com/tyler-romero" aria-label="GitHub"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.477 2 2 6.477 2 12c0 4.42 2.865 8.17 6.839 9.49.5.092.682-.217.682-.482 0-.237-.008-.866-.013-1.7-2.782.604-3.369-1.34-3.369-1.34-.454-1.156-1.11-1.463-1.11-1.463-.908-.62.069-.608.069-.608 1.003.07 1.531 1.03 1.531 1.03.892 1.529 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.11-4.555-4.943 0-1.091.39-1.984 1.029-2.683-.103-.253-.446-1.27.098-2.647 0 0 .84-.269 2.75 1.025A9.578 9.578 0 0112 6.836c.85.004 1.705.115 2.504.337 1.909-1.294 2.747-1.025 2.747-1.025.546 1.377.203 2.394.1 2.647.64.699 1.028 1.592 1.028 2.683 0 3.842-2.339 4.687-4.566 4.935.359.309.678.919.678 1.852 0 1.336-.012 2.415-.012 2.743 0 .267.18.578.688.48C19.138 20.167 22 16.418 22 12c0-5.523-4.477-10-10-10z"></path></svg></a>
        <a href="https://linkedin.com/in/tylerromero" aria-label="LinkedIn"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.062 2.062 0 01-2.063-2.065 2.064 2.064 0 112.063 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path></svg></a>
        <a href="https://scholar.google.com/citations?user=Hmrl0FAAAAAJ" aria-label="Google Scholar"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76 0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71 0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71 0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76 0 5-2.24 5-5s-2.24-5-5-5z"></path></svg></a>
        <a href="mailto:tyler.alexander.romero+site@gmail.com" aria-label="Email"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M20 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"></path></svg></a>
        <a href="/feed.xml" aria-label="RSS Feed"><svg class="icon" viewBox="0 0 24 24" fill="currentColor"><path d="M6.18 15.64a2.18 2.18 0 010 4.36 2.18 2.18 0 010-4.36zM4 4.44A15.56 15.56 0 0119.56 20h-2.83A12.73 12.73 0 004 7.27V4.44zm0 5.66a9.9 9.9 0 019.9 9.9h-2.83A7.07 7.07 0 004 12.93V10.1z"></path></svg></a>
    </p>
    <p class="copyright">&copy; 2026 Tyler Romero</p>

        </footer>
    </body>
</html>
