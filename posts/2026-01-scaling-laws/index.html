<!DOCTYPE html>
<html lang="en">
    <head>
        <!-- Required meta tags -->


<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Thinking about Scaling Laws</title>
<meta name="author" content="Tyler Romero">
<meta name="description" content="How can we use scaling laws to train stronger LLMs?">
<link rel="manifest" href="/manifest.json">
<link rel="icon" href="/assets/img/favicon.ico" type="image/ico">
<link rel="canonical" href="https://www.tylerromero.com/posts/2026-01-scaling-laws/">
<link type="application/atom+xml" rel="alternate" href="https://www.tylerromero.com/feed.xml" title="Tyler&#39;s Technical Blog">
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Crimson+Pro:ital,wght@0,300..700;1,300..700&display=swap">
<link rel="preload" as="font" type="font/woff2" crossorigin="" href="/assets/et-book/et-book-roman-old-style-figures/et-book-roman-old-style-figures.woff">
<!-- Open Graph -->
<meta property="og:title" content="Thinking about Scaling Laws">
<meta property="og:description" content="How can we use scaling laws to train stronger LLMs?">
<meta property="og:url" content="https://www.tylerromero.com/posts/2026-01-scaling-laws/">
<meta property="og:type" content="article">
<meta property="og:site_name" content="Tyler&#39;s Technical Blog">

<!-- Twitter Card -->
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Thinking about Scaling Laws">
<meta name="twitter:description" content="How can we use scaling laws to train stronger LLMs?">



<!-- KaTeX Support -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
<!-- BibTeX Support -->
<script type="text/javascript" src="https://cdn.jsdelivr.net/gh/pcooksey/bibtex-js@1.0.0/src/bibtex_js.min.js"></script>
<!-- Stylesheets -->
<link rel="stylesheet" href="/assets/tufte.min.css">
<link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css">
<!-- Goat Counter for basic view counting w/o cookies -->
<script data-goatcounter="https://tylerromero.goatcounter.com/count" async="" src="//gc.zgo.at/count.js"></script>

        
        <script type="application/ld+json">
            {
            "@context": "https://schema.org",
            "@type": "BlogPosting",
            "headline": "Thinking about Scaling Laws",
            "description": "How can we use scaling laws to train stronger LLMs?",
            "author": {
                "@type": "Person",
                "name": "Tyler Romero",
                "url": "https://www.tylerromero.com"
            },
            "datePublished": "2026-01-10T08:00:00.000Z",
            "dateModified": "2026-02-18T01:42:33.787Z",
            "url": "https://www.tylerromero.com/posts/2026-01-scaling-laws/",
            "mainEntityOfPage": {
                "@type": "WebPage",
                "@id": "https://www.tylerromero.com/posts/2026-01-scaling-laws/"
            },
            "image": "https://www.tylerromero.com",
            "keywords": "post, scaling-laws, chinchilla, llms",
            "wordCount": "2253",
            "articleBody": "Note: This post is a work in progress. I will continue to update and expand it over time. Scaling laws are one of the few tools we have for predicting model performance before committing serious compute. When a single training run can cost millions of dollars and take months, the ability to extrapolate from small-scale experiments becomes invaluable. In this post, I want to explore how we can use scaling laws not just as descriptive summaries, but as decision-making frameworks for comparing..."
            }
        </script>

    </head>
    <body>
        <header>
            <nav class="topbar">
                <div class="right-aligned-links">
                    <a href="/" class="no-tufte-underline nav-brand">Tyler Romero</a>
                    
                    <a href="/#posts" class="no-tufte-underline">posts</a>
                    <span class="desktop-only">
                        <a href="/#projects" class="no-tufte-underline">projects</a>
                        <a href="/#reading-list" class="no-tufte-underline">reading list</a>
                    </span>
                    
                </div>
            </nav>
        </header>
        <article>
            
            <div class="progress-bar"></div>
            <h1>Thinking about Scaling Laws
</h1>
            <p class="subtitle">How can we use scaling laws to train stronger LLMs?
</p>
            <p class="date">January 10, 2026</p>
            <script>
                (function() {
                    var bar = document.querySelector('.progress-bar');
                    window.addEventListener('scroll', function() {
                        var scrollTop = document.documentElement.scrollTop || document.body.scrollTop;
                        var scrollHeight = document.documentElement.scrollHeight - document.documentElement.clientHeight;
                        var progress = scrollHeight > 0 ? (scrollTop / scrollHeight) * 100 : 0;
                        bar.style.width = progress + '%';
                    });
                })();
            </script>

            <section><p><em>Note: This post is a work in progress. I will continue to update and expand it over time.</em></p><p>Scaling laws are one of the few tools we have for predicting model performance before committing serious compute. When a single training run can cost millions of dollars and take months, the ability to extrapolate from small-scale experiments becomes invaluable. In this post, I want to explore how we can use scaling laws not just as descriptive summaries, but as decision-making frameworks for comparing architectural choices and planning training runs.</p><p>This post assumes familiarity with the seminal papers <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> (Kaplan et al.) and <a href="https://arxiv.org/abs/2203.15556">Training Compute-Optimal Large Language Models</a> (Hoffmann et al.). If you haven’t read them, I’d recommend at least skimming Hoffmann et al. (the “Chinchilla” paper) first.</p></section>
<section><h2 id="the-chinchilla-scaling-law">The Chinchilla Scaling Law</h2><p>The scaling laws from Hoffmann et al., also known as the “Chinchilla” scaling laws, are the basis for the billions of dollars being spent on training larger models on more data. They are empirical predictions—validated across many orders of magnitude of scale—about how much compute (in terms of data and model size) is required to lower loss. Chinchilla also demonstrates that model size and data should be scaled up at roughly the same rate, giving rise to the well-known heuristic of “20 tokens per parameter” as the optimal ratio.</p><p>The Chinchilla paper uses three separate approaches to arrive at the same “compute-optimal” scaling results. In particular, the third approach models the final loss of a language model as a function of model size <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span></span> and training data <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span></span> in the following form:</p><div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>L</mi><mo>(</mo><mi>N</mi><mo separator="true">,</mo><mi>D</mi><mo>)</mo><mo>=</mo><mi>E</mi><mo>+</mo><mfrac><mi>A</mi><msup><mi>N</mi><mi>α</mi></msup></mfrac><mo>+</mo><mfrac><mi>B</mi><msup><mi>D</mi><mi>β</mi></msup></mfrac></mrow><annotation encoding="application/x-tex">L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">L</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.04633em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.0037em;">α</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.04633em;vertical-align:-0.686em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.36033em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7751079999999999em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.05278em;">β</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></div><p>Where:</p><ul>
<li><strong>N</strong> = number of model parameters</li>
<li><strong>D</strong> = number of training tokens</li>
<li><strong>E</strong> = irreducible loss — the theoretical minimum loss achievable with infinite compute</li>
<li><strong>A, α</strong> = parameter scaling coefficients (how loss improves with more parameters)</li>
<li><strong>B, β</strong> = data scaling coefficients (how loss improves with more training data)</li>
</ul><p>The parameters of this function are fit to empirical data.</p><p>The fitted values from the Chinchilla paper are approximately <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi><mo>≈</mo><mn>0.34</mn></mrow><annotation encoding="application/x-tex">\alpha \approx 0.34</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">4</span></span></span></span></span>, <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi><mo>≈</mo><mn>0.28</mn></mrow><annotation encoding="application/x-tex">\beta \approx 0.28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">8</span></span></span></span></span>, <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>E</mi><mo>≈</mo><mn>1.69</mn></mrow><annotation encoding="application/x-tex">E \approx 1.69</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05764em;">E</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span><span class="mord">.</span><span class="mord">6</span><span class="mord">9</span></span></span></span></span>, <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>A</mi><mo>≈</mo><mn>406.4</mn></mrow><annotation encoding="application/x-tex">A \approx 406.4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">0</span><span class="mord">6</span><span class="mord">.</span><span class="mord">4</span></span></span></span></span>, and <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>B</mi><mo>≈</mo><mn>410.7</mn></mrow><annotation encoding="application/x-tex">B \approx 410.7</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">4</span><span class="mord">1</span><span class="mord">0</span><span class="mord">.</span><span class="mord">7</span></span></span></span></span>.</p><p>The key insight comes from minimizing this loss function subject to a fixed compute budget <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span></span></span></span></span>. Since compute scales roughly as <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mo>∝</mo><mn>6</mn><mi>N</mi><mi>D</mi></mrow><annotation encoding="application/x-tex">C \propto 6ND</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">6</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span></span> (the number of FLOPs required for a forward and backward pass through a model of size <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span></span> for <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span></span> tokens), we can derive the optimal allocation of compute between model size and data.</p><p>Taking the derivative and setting it to zero, we find that the optimal scaling satisfies:</p><div class="math"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>∝</mo><msup><mi>C</mi><mi>a</mi></msup><mo separator="true">,</mo><mspace width="1em"></mspace><msub><mi>D</mi><mrow><mi>o</mi><mi>p</mi><mi>t</mi></mrow></msub><mo>∝</mo><msup><mi>C</mi><mi>b</mi></msup></mrow><annotation encoding="application/x-tex">N_{opt} \propto C^{a}, \quad D_{opt} \propto C^{b}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.0005em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">a</span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mspace" style="margin-right:1em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.28055599999999997em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8991079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">b</span></span></span></span></span></span></span></span></span></span></span></span></span></div><p>where <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>=</mo><mfrac><mi>β</mi><mrow><mi>α</mi><mo>+</mo><mi>β</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">a = \frac{\beta}{\alpha + \beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.4133239999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9322159999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.0037em;">α</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight" style="margin-right:0.05278em;">β</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.446108em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.05278em;">β</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> and <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>=</mo><mfrac><mi>α</mi><mrow><mi>α</mi><mo>+</mo><mi>β</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">b = \frac{\alpha}{\alpha + \beta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.1764999999999999em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.695392em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.0037em;">α</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight" style="margin-right:0.05278em;">β</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.0037em;">α</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>. Using the fitted values from Chinchilla (<span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi><mo>≈</mo><mn>0.34</mn></mrow><annotation encoding="application/x-tex">\alpha \approx 0.34</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">3</span><span class="mord">4</span></span></span></span></span>, <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>β</mi><mo>≈</mo><mn>0.28</mn></mrow><annotation encoding="application/x-tex">\beta \approx 0.28</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.05278em;">β</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">8</span></span></span></span></span>), we get <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>a</mi><mo>≈</mo><mn>0.45</mn></mrow><annotation encoding="application/x-tex">a \approx 0.45</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mord mathdefault">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">4</span><span class="mord">5</span></span></span></span></span> and <span class="inlineMath"><span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>b</mi><mo>≈</mo><mn>0.55</mn></mrow><annotation encoding="application/x-tex">b \approx 0.55</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mord">5</span></span></span></span></span>. This means that as compute increases, we should scale data slightly faster than model size.</p></section>
<section><h2 id="decision-making-with-scaling-laws">Decision Making with Scaling Laws</h2><p>The reason I dive into Chinchilla’s third approach is that a fitted function capable of predicting loss as a function of N and D is incredibly powerful for making real decisions about model development.</p><p>Let’s say we have a baseline model and a modeling change/intervention we want to test. Many researchers will test their change at 1–3 model scales on 1 or 2 different training dataset sizes, then conclude that their approach is better than the baseline on the back of these results. However, this does not tell the full story. A more correct way of conducting this experiment would be to fit empirical scaling laws based on results across many different model scales and training dataset sizes, and then compare the scaling laws to determine <em>in which regimes</em> their approach is better or worse than the baseline.</p><h3 id="scaling-behavior-efficiency-vs-scalability">Scaling Behavior: Efficiency vs. Scalability</h3><p>It’s important to distinguish between three different aspects of model performance:</p><p><strong>1. Offsets (A and B coefficients)</strong></p><ul>
<li>These coefficients are multiplicative constants that shift the scaling curve vertically</li>
<li>Lower A means lower loss for any given model size (a constant factor improvement in the parameter term)</li>
<li>Lower B means lower loss for any given dataset size (a constant factor improvement in the data term)</li>
<li>The optimal compute split between parameters and data is <em>independent</em> of A and B</li>
<li>A variant can have better offsets (lower A, B) while scaling at the same rate as baseline</li>
</ul><p><strong>2. Scalability (α and β exponents)</strong></p><ul>
<li>These exponents determine the <em>slope</em> of the scaling curve</li>
<li>A model with higher α and β will <em>eventually</em> beat one with lower exponents, regardless of A and B values (assuming a constant E)</li>
<li>Higher α means loss decreases faster as you add parameters</li>
<li>Higher β means loss decreases faster as you add data</li>
<li>A variant with worse scaling (lower α, β) will eventually be overtaken by baseline at large enough scale</li>
</ul><p><strong>3. Irreducible Loss (E)</strong></p><ul>
<li>At large scales, the A/N^α and B/D^β terms shrink toward zero, so E dominates.</li>
<li>Therefore, a lower E can indicate that a model will achieve a lower loss at large scale even if the marginal scaling efficiency is worse.</li>
<li>Despite there being a constant entropy floor for a given dataset, not all models can achieve the same irreducible loss on that dataset.</li>
</ul><p><strong>The Crossover Problem</strong></p><p>A variant that has <em>better offsets but scales worse</em> presents an interesting tradeoff:</p><ul>
<li>At small scale: Variant wins due to lower A and B</li>
<li>At large scale: Baseline wins due to better scaling (higher α, β)</li>
<li>The <strong>crossover point</strong> is where baseline catches up</li>
</ul><p><strong>When Scaling Differences Matter</strong></p><p>Small differences in α or β compound significantly at scale:</p><ul>
<li>A 1% difference in α over 1000× compute increase → ~7% difference in the parameter term</li>
<li>For frontier models (70B+ params, 10T+ tokens), even small α/β differences are meaningful</li>
</ul><p>The table below gives practical guidance on how to interpret differences in scaling parameters for different models.</p><table>
<thead>
<tr>
<th>α, β vs baseline</th>
<th>A, B vs baseline</th>
<th>E vs baseline</th>
<th>Verdict</th>
</tr>
</thead>
<tbody>
<tr>
<td>Higher (scales better)</td>
<td>Lower (better offsets)</td>
<td>Lower</td>
<td><strong>Best case</strong>
 — wins at all scales</td>
</tr>
<tr>
<td>Higher (scales better)</td>
<td>Lower (better offsets)</td>
<td>Higher</td>
<td>Wins at most scales, but baseline may catch up at very large scale due to E floor</td>
</tr>
<tr>
<td>Higher (scales better)</td>
<td>Higher (worse offsets)</td>
<td>Lower</td>
<td>Likely wins at large scale (better scaling + lower floor)</td>
</tr>
<tr>
<td>Higher (scales better)</td>
<td>Higher (worse offsets)</td>
<td>Higher</td>
<td>Mixed — scaling helps but E hurts; depends on target scale</td>
</tr>
<tr>
<td>Similar</td>
<td>Lower (better offsets)</td>
<td>Lower</td>
<td><strong>Good</strong>
 — consistent gains at all scales</td>
</tr>
<tr>
<td>Similar</td>
<td>Lower (better offsets)</td>
<td>Higher</td>
<td>Wins at small/medium scale, may lose at very large scale</td>
</tr>
<tr>
<td>Similar</td>
<td>Higher (worse offsets)</td>
<td>Lower</td>
<td>May recover at very large scale due to lower E</td>
</tr>
<tr>
<td>Similar</td>
<td>Higher (worse offsets)</td>
<td>Higher</td>
<td><strong>Bad</strong>
 — loses at all scales</td>
</tr>
<tr>
<td>Lower (scales worse)</td>
<td>Lower (better offsets)</td>
<td>Lower</td>
<td>Complex tradeoff — E helps at large scale but worse α/β hurts</td>
</tr>
<tr>
<td>Lower (scales worse)</td>
<td>Lower (better offsets)</td>
<td>Higher</td>
<td>Wins at small scale only — crossover point exists</td>
</tr>
<tr>
<td>Lower (scales worse)</td>
<td>Higher (worse offsets)</td>
<td>Lower</td>
<td>Only hope is very large scale where E dominates</td>
</tr>
<tr>
<td>Lower (scales worse)</td>
<td>Higher (worse offsets)</td>
<td>Higher</td>
<td><strong>Worst case</strong>
 — loses at all scales</td>
</tr>
</tbody>
</table><h3 id="targeting-specific-model-and-dataset-sizes">Targeting Specific Model and Dataset Sizes</h3><p>When pretraining models, there is essentially a set of model families of roughly the same size. For example, there are many 7B, 32B, and 70B parameter models. Additionally, labs know how many useful tokens they have available for pretraining (or roughly how many GPU hours can be allotted to a specific run).</p><p>Using a chosen model and dataset size (e.g., N=32B, D=10T), we can use our formula to predict the final training loss of a given model architecture based on our empirical scaling law fit. This makes decision making straightforward: whichever intervention predicts the lowest loss at the target scale is the one we should use<label for="sd-however-this-is" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sd-however-this-is" class="margin-toggle"><span class="sidenote">However, this is still ignoring other important considerations, such as inference-time efficiency, long-context performance, training throughput, and training stability. A complete evaluation framework would need to weigh these factors alongside the raw scaling predictions. For example, faster inference-time efficiency is critical for scaling up RL post-training and directly impacts the end-user experience.</span>.</p></section>
<section><h2 id="getting-strong-scaling-law-fits">Getting strong scaling law fits</h2><p>Coming soon.</p></section>
<section><h2 id="recommended-reading">Recommended Reading</h2><ul>
<li>
<p><a href="https://arxiv.org/abs/2406.19146">Resolving Discrepancies in Compute-Optimal Scaling of Language Models</a> — Investigates why different labs arrive at different scaling law coefficients and proposes methods to reconcile them.</p>
</li>
<li>
<p><a href="https://www.semanticscholar.org/paper/Chinchilla-Scaling%3A-A-replication-attempt-Besiroglu-Erdil/2cfe76f2fcb272fd0dde67b5468cdc462416fd38">Chinchilla Scaling: A replication attempt</a> — A thorough attempt to replicate Chinchilla’s results, revealing challenges and nuances in the original methodology.</p>
</li>
<li>
<p><a href="https://www.semanticscholar.org/paper/Beyond-Chinchilla-Optimal%3A-Accounting-for-Inference-Sardana-Doubov/82f75d838e92196864131bad25b1abc3b5d40a6f">Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws</a> — Argues that optimal training compute allocation should factor in inference costs, leading to smaller, more over-trained models.</p>
</li>
<li>
<p><a href="https://www.semanticscholar.org/paper/Scaling-Data-Constrained-Language-Models-Muennighoff-Rush/9e16d8cc6096ec0d2733a4ecf41ce09d9a4bd19c">Scaling Data-Constrained Language Models</a> — Explores what happens when you run out of unique data and must repeat tokens, with implications for data-constrained regimes.</p>
</li>
<li>
<p><a href="https://arxiv.org/abs/2510.02228">xLSTM Scaling Laws</a> — Useful case study that compares xLSTMs to Transformers using scaling laws.</p>
</li>
</ul><p><picture><source type="image/webp" srcset="/assets/img/UFnp5pZ7IP-300.webp 300w, /assets/img/UFnp5pZ7IP-600.webp 600w, /assets/img/UFnp5pZ7IP-900.webp 900w, /assets/img/UFnp5pZ7IP-2192.webp 2192w" sizes="(max-width: 900px) 100vw, 900px"><img loading="lazy" decoding="async" class="responsive-image" src="/assets/img/UFnp5pZ7IP-300.png" alt="From the xLSTM Scaling Laws paper." width="2192" height="996" srcset="/assets/img/UFnp5pZ7IP-300.png 300w, /assets/img/UFnp5pZ7IP-600.png 600w, /assets/img/UFnp5pZ7IP-900.png 900w, /assets/img/UFnp5pZ7IP-2192.png 2192w" sizes="(max-width: 900px) 100vw, 900px"></picture></p></section>

        </article>
        <footer>
                <hr class="slender">
    <p class="social-links">
        <a href="/" aria-label="Home"><span class="fas fa-home"></span></a>
        <a href="https://twitter.com/tyleraromero" aria-label="Twitter"><i class="fab fa-twitter-square"></i></a>
        <a href="https://github.com/tyler-romero" aria-label="GitHub"><i class="fab fa-github-square"></i></a>
        <a href="https://linkedin.com/in/tylerromero" aria-label="LinkedIn"><i class="fab fa-linkedin"></i></a>
        <a href="https://scholar.google.com/citations?user=Hmrl0FAAAAAJ" aria-label="Google Scholar"><i class="fas fa-link"></i></a>
        <a href="mailto:tyler.alexander.romero+site@gmail.com" aria-label="Email"><i class="fas fa-envelope-square"></i></a>
        <a href="/feed.xml" aria-label="RSS Feed"><i class="fas fa-rss-square"></i></a>
    </p>
    <p class="copyright">&copy; 2026 Tyler Romero</p>

        </footer>
    </body>
</html>
